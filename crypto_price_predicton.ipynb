{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435c77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import tensorflow.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348afee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         time        low       high       open      close      volume\n",
      "0  1528968660  96.580002  96.589996  96.589996  96.580002    9.647200\n",
      "1  1528968720  96.449997  96.669998  96.589996  96.660004  314.387024\n",
      "2  1528968780  96.470001  96.570000  96.570000  96.570000   77.129799\n",
      "3  1528968840  96.449997  96.570000  96.570000  96.500000    7.216067\n",
      "4  1528968900  96.279999  96.540001  96.500000  96.389999  524.539978\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/LTC-USD.csv\", names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d108a1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCH-USD\n",
      "         time         low        high        open       close     volume\n",
      "0  1528968660  871.650024  871.729980  871.650024  871.719971   5.675361\n",
      "1  1528968720  870.859985  871.719971  871.719971  870.859985  26.856577\n",
      "2  1528968780  870.099976  871.090027  871.090027  870.099976   1.124300\n",
      "3  1528968840  868.830017  870.950012  868.830017  870.789978   1.749862\n",
      "4  1528968900  870.000000  870.000000  870.000000  870.000000   1.680500\n",
      "BTC-USD\n",
      "         time          low         high         open        close    volume\n",
      "0  1528968660  6489.549805  6489.560059  6489.560059  6489.549805  0.587100\n",
      "1  1528968720  6487.370117  6489.560059  6489.549805  6487.379883  7.706374\n",
      "2  1528968780  6479.410156  6487.370117  6487.370117  6479.410156  3.088252\n",
      "3  1528968840  6479.410156  6479.419922  6479.419922  6479.410156  1.404100\n",
      "4  1528968900  6475.930176  6479.979980  6479.410156  6479.979980  0.753000\n",
      "LTC-USD\n",
      "         time        low       high       open      close      volume\n",
      "0  1528968660  96.580002  96.589996  96.589996  96.580002    9.647200\n",
      "1  1528968720  96.449997  96.669998  96.589996  96.660004  314.387024\n",
      "2  1528968780  96.470001  96.570000  96.570000  96.570000   77.129799\n",
      "3  1528968840  96.449997  96.570000  96.570000  96.500000    7.216067\n",
      "4  1528968900  96.279999  96.540001  96.500000  96.389999  524.539978\n",
      "ETH-USD\n",
      "         time        low   high        open      close     volume\n",
      "0  1528968720  485.98999  486.5  486.019989  486.01001  26.019083\n",
      "1  1528968780  486.00000  486.0  486.000000  486.00000   8.449400\n",
      "2  1528968840  485.75000  486.0  486.000000  485.75000  26.994646\n",
      "3  1528968900  485.75000  486.0  485.750000  486.00000  77.355759\n",
      "4  1528968960  485.98999  486.0  486.000000  486.00000   7.503300\n"
     ]
    }
   ],
   "source": [
    "currencies=[\"BCH-USD\",\"BTC-USD\",\"LTC-USD\",\"ETH-USD\"]\n",
    "for cur in currencies:\n",
    "    df=pd.read_csv(f\"datasets/{cur}.csv\", names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "    print(cur)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c09f9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 60  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"LTC-USD\"\n",
    "\n",
    "\n",
    "def label(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1053149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BCH-USD_close  BCH-USD_volume  BTC-USD_close  BTC-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968720     870.859985       26.856577    6487.379883        7.706374   \n",
      "1528968780     870.099976        1.124300    6479.410156        3.088252   \n",
      "1528968840     870.789978        1.749862    6479.410156        1.404100   \n",
      "1528968900     870.000000        1.680500    6479.979980        0.753000   \n",
      "1528968960     869.989990        1.669014    6480.000000        1.490900   \n",
      "\n",
      "            LTC-USD_close  LTC-USD_volume  ETH-USD_close  ETH-USD_volume  \n",
      "time                                                                      \n",
      "1528968720      96.660004      314.387024      486.01001       26.019083  \n",
      "1528968780      96.570000       77.129799      486.00000        8.449400  \n",
      "1528968840      96.500000        7.216067      485.75000       26.994646  \n",
      "1528968900      96.389999      524.539978      486.00000       77.355759  \n",
      "1528968960      96.519997       16.991997      486.00000        7.503300  \n"
     ]
    }
   ],
   "source": [
    "full_df = pd.DataFrame()\n",
    "currencies=[\"BCH-USD\",\"BTC-USD\",\"LTC-USD\",\"ETH-USD\"]\n",
    "for curr in currencies:\n",
    "    df=pd.read_csv(f\"datasets/{curr}.csv\", names=['time', 'low', 'high', 'open', 'close', 'volume'])\n",
    "    df.rename(columns={\"close\": f\"{curr}_close\", \"volume\": f\"{curr}_volume\"}, inplace=True)\n",
    "    df.set_index(\"time\", inplace=True)  \n",
    "    df = df.loc[:,[f\"{curr}_close\", f\"{curr}_volume\"]]\n",
    "\n",
    "    if len(full_df)==0:  # if the dataframe is empty\n",
    "        full_df = df \n",
    "    else:  \n",
    "        full_df = full_df.join(df)\n",
    "\n",
    "full_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
    "full_df.dropna(inplace=True)\n",
    "print(full_df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db53bec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BCH-USD_close  BCH-USD_volume  BTC-USD_close  BTC-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968720     870.859985       26.856577    6487.379883        7.706374   \n",
      "1528968780     870.099976        1.124300    6479.410156        3.088252   \n",
      "1528968840     870.789978        1.749862    6479.410156        1.404100   \n",
      "1528968900     870.000000        1.680500    6479.979980        0.753000   \n",
      "1528968960     869.989990        1.669014    6480.000000        1.490900   \n",
      "\n",
      "            LTC-USD_close  LTC-USD_volume  ETH-USD_close  ETH-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968720      96.660004      314.387024      486.01001       26.019083   \n",
      "1528968780      96.570000       77.129799      486.00000        8.449400   \n",
      "1528968840      96.500000        7.216067      485.75000       26.994646   \n",
      "1528968900      96.389999      524.539978      486.00000       77.355759   \n",
      "1528968960      96.519997       16.991997      486.00000        7.503300   \n",
      "\n",
      "               future  target  \n",
      "time                           \n",
      "1528968720  96.389999       0  \n",
      "1528968780  96.519997       0  \n",
      "1528968840  96.440002       0  \n",
      "1528968900  96.470001       1  \n",
      "1528968960  96.400002       0  \n"
     ]
    }
   ],
   "source": [
    "full_df['future'] = full_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "full_df['target'] = list(map(label, full_df[f'{RATIO_TO_PREDICT}_close'], full_df['future']))\n",
    "\n",
    "print(full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "491df3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sorted(full_df.index.values)  # get the times\n",
    "last_10pct_timestamp= sorted(full_df.index.values)[-int(0.1*len(times))]  #last 10% timestamp in times\n",
    "last_20pct_timestamp= sorted(full_df.index.values)[-int(0.2*len(times))]\n",
    "\n",
    "train_full_df = full_df[(full_df.index < last_20pct_timestamp)] #80% train data\n",
    "validation_full_df = full_df[(full_df.index >= last_20pct_timestamp) & (full_df.index < last_10pct_timestamp)] #10% validation\n",
    "test_full_df = full_df[(full_df.index >= last_10pct_timestamp)]# 10% test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b1bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(crpyto_data_df):\n",
    "    df = crpyto_data_df.drop(\"future\", axis=1)  # we can drop it now as it was used to calculate target\n",
    "\n",
    "    for col in df.columns:  # go through all of the columns\n",
    "        if col != \"target\":  # normalize all ... except for the target itself!\n",
    "            df[col] = df[col].pct_change()  # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\n",
    "            df.dropna(inplace=True)  # remove the nas created by pct_change\n",
    "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
    "\n",
    "    df.dropna(inplace=True)  # cleanup again... jic.\n",
    "\n",
    "\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in\n",
    "\n",
    "    for i in df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "\n",
    "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "\n",
    "    buys = []  # list that will store our buy sequences and targets\n",
    "    sells = []  # list that will store our sell sequences and targets\n",
    "\n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        if target == 0:  # if it's a \"not buy\"\n",
    "            sells.append([seq, target])  # append to sells list\n",
    "        elif target == 1:  # otherwise if the target is a 1...\n",
    "            buys.append([seq, target])  # it's a buy!\n",
    "\n",
    "    random.shuffle(buys)  # shuffle the buys\n",
    "    random.shuffle(sells)  # shuffle the sells!\n",
    "\n",
    "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
    "\n",
    "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
    "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
    "\n",
    "    sequential_data = buys+sells  # add them together\n",
    "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
    "\n",
    "    return np.array(X), y  # return X and y...and make X a numpy array!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e205de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = preprocess_df(train_full_df)\n",
    "validation_x, validation_y = preprocess_df(validation_full_df)\n",
    "test_x, test_y = preprocess_df(test_full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affa85a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 62434 validation: 8522  test: 7810\n",
      "train data dont buys: 31217, buys: 31217\n",
      "validation Dont buys: 4261, buys: 4261\n",
      "test data Dont buys: 3905, buys: 3905\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}  test: {len(test_x)}\")\n",
    "print(f\"train data dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"validation Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n",
    "print(f\"test data Dont buys: {test_y.count(0)}, buys: {test_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b97a00",
   "metadata": {},
   "source": [
    "## CNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454e560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f3a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the desired EPOCHS, BATCH_SIZES and NAME of the model to be saved\n",
    "\n",
    "EPOCHS = 30  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "num_classes = 2 # buy and sell\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"  # a unique name for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d89663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "976/976 [==============================] - 578s 586ms/step - loss: 0.7216 - accuracy: 0.5152 - val_loss: 0.6931 - val_accuracy: 0.5224\n",
      "Epoch 2/30\n",
      "976/976 [==============================] - 552s 565ms/step - loss: 0.6895 - accuracy: 0.5376 - val_loss: 0.6917 - val_accuracy: 0.5352\n",
      "Epoch 3/30\n",
      "976/976 [==============================] - 586s 601ms/step - loss: 0.6848 - accuracy: 0.5542 - val_loss: 0.6877 - val_accuracy: 0.5547\n",
      "Epoch 4/30\n",
      "976/976 [==============================] - 577s 591ms/step - loss: 0.6811 - accuracy: 0.5660 - val_loss: 0.6882 - val_accuracy: 0.5387\n",
      "Epoch 5/30\n",
      "976/976 [==============================] - 581s 595ms/step - loss: 0.6789 - accuracy: 0.5692 - val_loss: 0.6959 - val_accuracy: 0.5426\n",
      "Epoch 6/30\n",
      "976/976 [==============================] - 552s 566ms/step - loss: 0.6768 - accuracy: 0.5739 - val_loss: 0.6949 - val_accuracy: 0.5466\n",
      "Epoch 7/30\n",
      "976/976 [==============================] - 587s 602ms/step - loss: 0.6742 - accuracy: 0.5814 - val_loss: 0.6873 - val_accuracy: 0.5433\n",
      "Epoch 8/30\n",
      "976/976 [==============================] - 612s 627ms/step - loss: 0.6705 - accuracy: 0.5897 - val_loss: 0.6973 - val_accuracy: 0.5448\n",
      "Epoch 9/30\n",
      "976/976 [==============================] - 535s 547ms/step - loss: 0.6624 - accuracy: 0.5982 - val_loss: 0.6963 - val_accuracy: 0.5404\n",
      "Epoch 10/30\n",
      "411/976 [===========>..................] - ETA: 4:19 - loss: 0.6510 - accuracy: 0.6188"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAKESH~1.DOD\\AppData\\Local\\Temp/ipykernel_19444/2268995566.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1407\u001b[0m                 _r=1):\n\u001b[0;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1410\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = Sequential([\n",
    "    layers.LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.BatchNormalization(), #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "    \n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.LSTM(128),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{validation_accuracy:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "#checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='validation_accuracy', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)\n",
    "validation_x = np.asarray(validation_x)\n",
    "validation_y = np.asarray(validation_y)\n",
    "test_x = np.asarray(test_x)\n",
    "test_y = np.asarray(test_y)\n",
    "\n",
    "model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard]\n",
    "    #callbacks=[tensorboard, checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ba0771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7024799585342407, 0.5425096154212952]\n",
      "Test loss: 0.7024799585342407\n",
      "Test accuracy: 0.5425096154212952\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(score)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ab73b",
   "metadata": {},
   "source": [
    "To improve the test accuracy increase the dataset and epochs while training the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc561b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/60-SEQ-3-PRED-1657273572\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/60-SEQ-3-PRED-1657273572\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save(\"models/{}\".format(NAME))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
